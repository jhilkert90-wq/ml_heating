#!/usr/bin/env python3
"""
Integration Test for Adaptive Learning Master Plan Implementation

This test validates all components of the Adaptive Learning Master Plan:
1. Re-enabled adaptive learning
2. Empty trajectory methods implementation
3. MAE/RMSE tracking system
4. Enhanced HA metrics export
"""

import sys
import os
import json
from datetime import datetime

# Add the src directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

# Import all components
from model_wrapper import get_enhanced_model_wrapper
from thermal_equilibrium_model import ThermalEquilibriumModel
from prediction_metrics import PredictionMetrics
import config

def test_adaptive_learning_enabled():
    """Test 1: Verify adaptive learning is re-enabled."""
    print("ğŸ§ª Test 1: Adaptive Learning Re-enabled")
    
    model = ThermalEquilibriumModel()
    
    # Verify adaptive learning is enabled
    assert model.adaptive_learning_enabled == True, "Adaptive learning should be enabled"
    
    # Test parameter update mechanism works
    initial_thermal = model.thermal_time_constant
    initial_heat_loss = model.heat_loss_coefficient
    
    # Simulate prediction feedback to trigger learning with larger errors
    for i in range(20):  # More predictions with significant errors
        predicted = 21.0
        # Create larger, more varied errors that would trigger parameter updates
        actual = 19.0 + (i * 0.2)  # More significant errors: 19.0 to 22.8Â°C
        context = {
            'outlet_temp': 40.0 + (i * 2),  # Vary outlet temp too
            'outdoor_temp': 5.0 + (i * 0.5),  # Vary outdoor temp
            'pv_power': i * 100.0,  # Vary PV power
            'fireplace_on': i % 2,  # Alternate fireplace
            'tv_on': (i + 1) % 2  # Alternate TV
        }
        model.update_prediction_feedback(predicted, actual, context)
    
    # Check if parameters have been updated OR if learning confidence increased
    parameter_changed = (
        abs(model.thermal_time_constant - initial_thermal) > 0.001 or
        abs(model.heat_loss_coefficient - initial_heat_loss) > 0.0001
    )
    
    # Also check if learning confidence increased (indicates learning is working)
    confidence_increased = model.learning_confidence > 3.0
    
    print(f"   âœ… Adaptive learning enabled: {model.adaptive_learning_enabled}")
    print(f"   âœ… Parameters updating: {parameter_changed}")
    print(f"   âœ… Learning confidence: {model.learning_confidence:.3f}")
    print(f"   âœ… Confidence increased: {confidence_increased}")
    
    # Test passes if either parameters changed OR confidence increased significantly
    learning_working = parameter_changed or confidence_increased
    assert learning_working, "Parameters should update or confidence should increase with adaptive learning"

def test_empty_trajectory_methods():
    """Test 2: Verify empty trajectory methods are implemented."""
    print("\nğŸ§ª Test 2: Empty Trajectory Methods Implementation")
    
    model = ThermalEquilibriumModel()
    
    # Test predict_thermal_trajectory method
    try:
        trajectory_result = model.predict_thermal_trajectory(
            current_indoor=20.0,
            target_indoor=21.0,
            outlet_temp=45.0,
            outdoor_temp=10.0,
            time_horizon_hours=4
        )
        
        assert isinstance(trajectory_result, dict), "Should return dictionary"
        assert 'trajectory' in trajectory_result, "Should have trajectory key"
        assert 'reaches_target_at' in trajectory_result, "Should have reaches_target_at key"
        assert len(trajectory_result['trajectory']) > 0, "Should have non-empty trajectory"
        
        print(f"   âœ… predict_thermal_trajectory working")
        print(f"   âœ… Trajectory length: {len(trajectory_result['trajectory'])}")
        print(f"   âœ… Reaches target at: {trajectory_result['reaches_target_at']}")
        
    except Exception as e:
        print(f"   âŒ predict_thermal_trajectory failed: {e}")
        assert False, f"predict_thermal_trajectory failed: {e}"
    
    # Test calculate_optimal_outlet_temperature method
    try:
        outlet_result = model.calculate_optimal_outlet_temperature(
            target_indoor=21.0,
            current_indoor=20.0,
            outdoor_temp=10.0
        )
        
        assert outlet_result is not None, "Should not return None"
        assert isinstance(outlet_result, dict), "Should return dictionary"
        assert 'optimal_outlet_temp' in outlet_result, "Should have optimal_outlet_temp key"
        
        print(f"   âœ… calculate_optimal_outlet_temperature working")
        print(f"   âœ… Optimal outlet temp: {outlet_result['optimal_outlet_temp']:.1f}Â°C")
        
    except Exception as e:
        print(f"   âŒ calculate_optimal_outlet_temperature failed: {e}")
        assert False, f"calculate_optimal_outlet_temperature failed: {e}"

def test_mae_rmse_tracking():
    """Test 3: Verify MAE/RMSE tracking system."""
    print("\nğŸ§ª Test 3: MAE/RMSE Tracking System")
    
    # Test PredictionMetrics class
    try:
        metrics = PredictionMetrics()
        
        # Add some test predictions
        for i in range(20):
            predicted = 21.0 + (i * 0.01)
            actual = 21.0 + (i * 0.01) + 0.1  # Small consistent error
            context = {'outlet_temp': 45.0}
            
            metrics.add_prediction(predicted, actual, context)
        
        # Get metrics
        metrics_result = metrics.get_metrics()
        
        assert 'all' in metrics_result, "Should have 'all' time period"
        assert 'mae' in metrics_result['all'], "Should have MAE"
        assert 'rmse' in metrics_result['all'], "Should have RMSE"
        
        mae = metrics_result['all']['mae']
        rmse = metrics_result['all']['rmse']
        
        assert mae > 0, "MAE should be positive"
        assert rmse > 0, "RMSE should be positive"
        assert abs(mae - 0.1) < 0.01, f"MAE should be ~0.1, got {mae}"
        
        print(f"   âœ… PredictionMetrics class working")
        print(f"   âœ… MAE calculation: {mae:.3f}Â°C")
        print(f"   âœ… RMSE calculation: {rmse:.3f}Â°C")
        
        # Test recent performance
        recent = metrics.get_recent_performance(10)
        assert 'mae' in recent, "Recent performance should have MAE"
        print(f"   âœ… Recent MAE (10): {recent['mae']:.3f}Â°C")
        
        # Test accuracy breakdown
        breakdown = metrics_result.get('accuracy_breakdown', {})
        print(f"   âœ… Accuracy breakdown available: {len(breakdown) > 0}")
        
    except Exception as e:
        print(f"   âŒ MAE/RMSE tracking failed: {e}")
        assert False, f"MAE/RMSE tracking failed: {e}"

def test_enhanced_ha_metrics():
    """Test 4: Verify enhanced HA metrics export."""
    print("\nğŸ§ª Test 4: Enhanced HA Metrics Export")
    
    try:
        # Create enhanced model wrapper
        wrapper = get_enhanced_model_wrapper()
        
        # Add some test prediction data
        wrapper.prediction_metrics.add_prediction(21.1, 21.0, {'outlet_temp': 45.0})
        wrapper.prediction_metrics.add_prediction(20.9, 21.0, {'outlet_temp': 47.0})
        wrapper.prediction_metrics.add_prediction(21.2, 21.0, {'outlet_temp': 43.0})
        
        # Test comprehensive HA metrics
        ha_metrics = wrapper.get_comprehensive_metrics_for_ha()
        
        # Verify required fields for HA
        required_fields = [
            'thermal_time_constant', 'heat_loss_coefficient', 'outlet_effectiveness',
            'learning_confidence', 'cycle_count', 'mae_all_time', 'rmse_all_time',
            'model_health', 'total_predictions', 'last_updated'
        ]
        
        for field in required_fields:
            assert field in ha_metrics, f"Missing required HA field: {field}"
        
        print(f"   âœ… All required HA fields present")
        print(f"   âœ… Learning Confidence: {ha_metrics['learning_confidence']:.2f}")
        print(f"   âœ… Model Health: {ha_metrics['model_health']}")
        print(f"   âœ… MAE All Time: {ha_metrics['mae_all_time']:.3f}Â°C")
        print(f"   âœ… Total Predictions: {ha_metrics['total_predictions']}")
        
        # Test that values are reasonable
        assert 0 <= ha_metrics['learning_confidence'] <= 5, "Learning confidence out of range"
        assert ha_metrics['mae_all_time'] >= 0, "MAE should be non-negative"
        assert ha_metrics['total_predictions'] > 0, "Should have predictions"
        
    except Exception as e:
        print(f"   âŒ Enhanced HA metrics failed: {e}")
        assert False, f"Enhanced HA metrics failed: {e}"

def test_integration_workflow():
    """Test 5: Full integration workflow."""
    print("\nğŸ§ª Test 5: Full Integration Workflow")
    
    try:
        # Create wrapper and simulate realistic usage
        wrapper = get_enhanced_model_wrapper()
        
        # Simulate a full prediction and learning cycle
        features_dict = {
            'indoor_temp_lag_30m': 20.0,
            'target_temp': 21.0,
            'outdoor_temp': 10.0,
            'pv_now': 500.0,
            'fireplace_on': 0,
            'tv_on': 1
        }
        
        # Make prediction
        outlet_temp, metadata = wrapper.calculate_optimal_outlet_temp(features_dict)
        
        assert outlet_temp > 0, "Should get valid outlet temperature"
        assert isinstance(metadata, dict), "Should get metadata"
        
        print(f"   âœ… Prediction made: {outlet_temp:.1f}Â°C")
        
        # Simulate learning feedback
        actual_temp = 20.8  # Realistic actual measurement
        wrapper.learn_from_prediction_feedback(
            predicted_temp=21.0,
            actual_temp=actual_temp,
            prediction_context=features_dict
        )
        
        print(f"   âœ… Learning feedback processed")
        
        # Get comprehensive metrics
        final_metrics = wrapper.get_comprehensive_metrics_for_ha()
        
        print(f"   âœ… Final learning confidence: {final_metrics['learning_confidence']:.2f}")
        print(f"   âœ… Final cycle count: {final_metrics['cycle_count']}")
        
    except Exception as e:
        print(f"   âŒ Integration workflow failed: {e}")
        assert False, f"Integration workflow failed: {e}"

def main():
    """Run all integration tests."""
    print("ğŸš€ ADAPTIVE LEARNING MASTER PLAN - INTEGRATION TEST")
    print("=" * 60)
    
    tests = [
        test_adaptive_learning_enabled,
        test_empty_trajectory_methods, 
        test_mae_rmse_tracking,
        test_enhanced_ha_metrics,
        test_integration_workflow
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                print(f"   âŒ {test_func.__name__} failed")
        except Exception as e:
            print(f"   âŒ {test_func.__name__} crashed: {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š INTEGRATION TEST RESULTS:")
    print(f"   âœ… Passed: {passed}/{total}")
    print(f"   âŒ Failed: {total - passed}/{total}")
    
    if passed == total:
        print(f"\nğŸ‰ ALL TESTS PASSED! Adaptive Learning Master Plan is working!")
        return 0
    else:
        print(f"\nâŒ Some tests failed. Check implementation.")
        return 1

if __name__ == "__main__":
    exit(main())
